{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Processing for Arabic - Corpus Statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "In this exercise we produce several statistics from a given corpus. The corpus can be in any number of representations.\n",
    "\n",
    "We present the following statistics: \n",
    "1. Number of lines.\n",
    "2. Number of tokens.\n",
    "3. Number of unique tokens.\n",
    "4. Top 10 most frequent tokens.\n",
    "4. Full historgram (written to a file with `.freq.tsv` extension).\n",
    "\n",
    "When running a small file that contains the following:\n",
    "```\n",
    "العقل السليم في الجسم السليم\n",
    "```\n",
    "We expect that we will see the following statistics:\n",
    "\n",
    "Number of lines = 1\n",
    "\n",
    "Number of tokens = 5\n",
    "```\n",
    "['العقل', 'السليم', 'في', 'الجسم', 'السليم']\n",
    "```\n",
    "\n",
    "Number of unique tokens = 4  \n",
    "```\n",
    "['العقل', 'في', 'الجسم', 'السليم']\n",
    "```\n",
    "\n",
    "The top 10 most frequent tokens (this example will only have four):\n",
    "```\n",
    "Freq     Token\n",
    "2        السليم\n",
    "1        العقل\n",
    "1        في\n",
    "1        الجسم\n",
    "```\n",
    "\n",
    "To generate the statistics for a given corpus:\n",
    "```\n",
    "corpus_statistics(corpus_file)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter \n",
    "from camel_tools.utils.normalize import normalize_unicode\n",
    "import csv\n",
    "\n",
    "# function to get a list of sentences from a raw file\n",
    "def get_sentences_from_raw(filename):\n",
    "    sentences = []\n",
    "\n",
    "    # Open the file for reading, assuming it is UTF-8 encoded\n",
    "    with open(filename, mode='r', encoding='utf8') as input_file:\n",
    "\n",
    "        # Iterate through every line in the file\n",
    "        for line in input_file:\n",
    "\n",
    "            # Normalize unicode characters\n",
    "            normalized_sentence = normalize_unicode(line)\n",
    "            \n",
    "            # Remove spaces/tabs/newlines at the beginning and end of the sentence\n",
    "            stripped_line = normalized_sentence.strip()\n",
    "\n",
    "            # Add the sentence to the existing list of sentences\n",
    "            sentences.append(stripped_line)\n",
    "\n",
    "    return sentences\n",
    "\n",
    "# function to generate different statistics\n",
    "def corpus_statistics(filename):\n",
    "        \n",
    "    # laod the file into a list object\n",
    "    sentences = get_sentences_from_raw(filename)\n",
    "    \n",
    "    # initialize an empty list to store the word tokens\n",
    "    list_of_tokens = []\n",
    "    \n",
    "    for sentence in sentences:\n",
    "        \n",
    "        # for every sentence in the list, extract the words and append them to the list \n",
    "        list_of_tokens.extend(sentence.split('))\n",
    "        \n",
    "    # number of sentences\n",
    "    num_of_sentences = len(sentences) \n",
    "    \n",
    "    # number of words\n",
    "    num_of_tokens = len(list_of_tokens)\n",
    "    \n",
    "    # generate a histogram from the complete list of words (the frequency for each unique word)\n",
    "    histogram = Counter(list_of_tokens)\n",
    "    \n",
    "    # sort the histogram according to the highest frequency\n",
    "    sorted_histogram = {k: v for k, v in sorted(histogram.items(), key=lambda item: item[1], reverse=2)}\n",
    "    \n",
    "    # print the statitics to the screen\n",
    "    print('Number of lines:\\t{}'.format(num_of_sentences))\n",
    "    print('Number or tokens:\\t{}'.format(num_of_tokens))\n",
    "    print('Number of unique tokens:\\t{}'.format(len(histogram)))\n",
    "    \n",
    "    # get the top 10 most frequent unique words\n",
    "    print('Top 10 most frequent tokens:')\n",
    "    for word in list(sorted_histogram)[0:10]:\n",
    "        print('\\t{}\\t{}'.format(word, sorted_histogram[word]))\n",
    "    \n",
    "    # write histogram to file:\n",
    "    with open(filename+'.freq.tsv', 'w', encoding='utf-8', newline='') as csvfile:\n",
    "        \n",
    "        # create a writer object\n",
    "        row_writer = csv.writer(csvfile, dialect='excel-tab')\n",
    "        \n",
    "        # write the header of the table\n",
    "        row_writer.writerow(['Token', 'Freq'])\n",
    "        for word in sorted_histogram:\n",
    "            # write the rows (row by row)\n",
    "            row_writer.writerow([word, sorted_histogram[word]])\n",
    "    print('A complete historgram is written to \\'{}.freq.tsv\\''.format(filename))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Corpus Statistics: Gigaword_AR lemmas as an example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_statistics('Results/Gigaword_AR/gigaword_tiny_cleaned.txt.ATB.tok')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
